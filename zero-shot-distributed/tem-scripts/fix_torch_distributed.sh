#!/bin/bash

echo "=================================================="
echo "  PyTorchåˆ†å¸ƒå¼å®Œæ•´ä¿®å¤å·¥å…·ï¼ˆäº‘ç«¯æ‰§è¡Œï¼‰"
echo "=================================================="
echo "ğŸ¯ ä¿®å¤ç›®æ ‡: torch.distributed.nn æ¨¡å—é—®é¢˜ + å¯åŠ¨å‘½ä»¤"
echo "ğŸ”§ è§£å†³é—®é¢˜: PyTorch 1.12.0+cu113 å®Œæ•´å…¼å®¹æ€§ä¿®å¤"
echo "ğŸŒ¥ï¸  æ‰§è¡Œä½ç½®: seetacloud-v800 æœåŠ¡å™¨"
echo ""

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æœåŠ¡å™¨é…ç½®
SERVER="seetacloud-v800"

echo "ğŸ”— è¿æ¥åˆ°äº‘ç«¯æœåŠ¡å™¨: $SERVER"
echo ""

# æ£€æŸ¥SSHè¿æ¥
echo "ğŸ“¡ æµ‹è¯•SSHè¿æ¥..."
if ! ssh $SERVER "echo 'âœ… SSHè¿æ¥æˆåŠŸ'" 2>/dev/null; then
    echo -e "${RED}âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ $SERVER${NC}"
    echo "è¯·æ£€æŸ¥:"
    echo "  1. SSHé…ç½®æ˜¯å¦æ­£ç¡®"
    echo "  2. æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ"
    echo "  3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸"
    exit 1
fi

echo ""
echo "ğŸ“¤ ä¸Šä¼ å®Œæ•´ä¿®å¤è„šæœ¬åˆ°æœåŠ¡å™¨..."

# åˆ›å»ºä¿®å¤è„šæœ¬å†…å®¹ï¼ˆåµŒå…¥å®Œæ•´çš„Pythonä¿®å¤è„šæœ¬ï¼‰
cat > /tmp/fix_torch_complete_remote.py << 'EOF'
#!/usr/bin/env python3
"""
å®Œæ•´ä¿®å¤ torch.distributed.nn é—®é¢˜
è§£å†³ PyTorch 1.12.0 ä¸­ç¼ºå¤±çš„æ¨¡å—å’Œå‡½æ•°
"""

import os
import re
import shutil
from pathlib import Path

def fix_broken_syntax():
    """ä¿®å¤ç”±äºé‡å¤tryè¯­å¥é€ æˆçš„è¯­æ³•é”™è¯¯"""
    
    train_file = "/root/autodl-tmp/Chinese-CLIP/cn_clip/training/train.py"
    
    if os.path.exists(train_file):
        print(f"ğŸ”§ æ£€æŸ¥å¹¶ä¿®å¤ {train_file} çš„è¯­æ³•é”™è¯¯...")
        
        with open(train_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        original_content = content
        
        # ä¿®å¤é‡å¤çš„tryè¯­å¥
        content = re.sub(r'try:\s*try:', 'try:', content, flags=re.MULTILINE)
        
        # ä¿®å¤ç¼©è¿›é—®é¢˜å’Œå¤šé‡tryå—
        lines = content.split('\n')
        fixed_lines = []
        skip_next = False
        
        for i, line in enumerate(lines):
            if skip_next:
                skip_next = False
                continue
                
            stripped_line = line.strip()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„tryè¯­å¥
            if 'try:' in line and i < len(lines) - 1:
                next_line = lines[i + 1].strip()
                if next_line == 'try:':
                    # è·³è¿‡é‡å¤çš„try
                    skip_next = True
            
            fixed_lines.append(line)
        
        content = '\n'.join(fixed_lines)
        
        # æ¸…ç†å¤šä½™çš„å…¼å®¹æ€§ä¿®å¤ä»£ç å—
        compatibility_pattern = r'# PyTorch 1\.12\.0 å…¼å®¹æ€§ä¿®å¤.*?print\("âœ… torch\.distributed\.nn å…¼å®¹å±‚å·²åˆ›å»º"\)'
        matches = list(re.finditer(compatibility_pattern, content, re.DOTALL))
        
        if len(matches) > 1:
            print("ğŸ§¹ æ¸…ç†é‡å¤çš„å…¼å®¹æ€§ä»£ç å—...")
            # ä¿ç•™ç¬¬ä¸€ä¸ªï¼Œåˆ é™¤å…¶ä»–çš„
            for match in reversed(matches[1:]):
                content = content[:match.start()] + content[match.end():]
        
        if content != original_content:
            # åˆ›å»ºå¤‡ä»½
            backup_path = train_file + '.backup_syntax_fix'
            shutil.copy2(train_file, backup_path)
            
            # å†™å…¥ä¿®å¤åçš„å†…å®¹
            with open(train_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
            print(f"âœ… å·²ä¿®å¤è¯­æ³•é”™è¯¯: {train_file}")
            return True
    
    return False

def fix_torch_distributed_nn_imports():
    """ä¿®å¤æ‰€æœ‰ torch.distributed.nn ç›¸å…³çš„å¯¼å…¥å’Œä½¿ç”¨é—®é¢˜"""
    
    # æŸ¥æ‰¾æ‰€æœ‰éœ€è¦ä¿®å¤çš„ Python æ–‡ä»¶
    python_files = []
    
    # æœç´¢ç›®å½•
    search_dirs = [
        "/root/autodl-tmp/Chinese-CLIP",
        "/root/autodl-tmp/cn_clip",
    ]
    
    for search_dir in search_dirs:
        if os.path.exists(search_dir):
            for root, dirs, files in os.walk(search_dir):
                for file in files:
                    if file.endswith('.py'):
                        python_files.append(os.path.join(root, file))
    
    print(f"ğŸ” æ‰¾åˆ° {len(python_files)} ä¸ª Python æ–‡ä»¶éœ€è¦æ£€æŸ¥")
    
    fixed_files = []
    
    for file_path in python_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            original_content = content
            
            # åªå¯¹åŒ…å«torch.distributed.nnä½†æ²¡æœ‰å…¼å®¹æ€§ä¿®å¤çš„æ–‡ä»¶è¿›è¡Œå¤„ç†
            if 'torch.distributed.nn' in content and 'å…¼å®¹æ€§ä¿®å¤' not in content:
                print(f"ğŸ“ ä¸º {file_path} æ·»åŠ å…¼å®¹æ€§åŒ…è£…")
                
                # ç®€åŒ–çš„å…¼å®¹æ€§ä»£ç ï¼Œé¿å…è¯­æ³•é”™è¯¯
                compatibility_code = '''# PyTorch 1.12.0 å…¼å®¹æ€§ä¿®å¤
try:
    import torch.distributed.nn
except (ImportError, ModuleNotFoundError):
    import torch.distributed
    import types
    nn_module = types.ModuleType('torch.distributed.nn')
    nn_module.all_gather = torch.distributed.all_gather
    torch.distributed.nn = nn_module

'''
                
                # åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ å…¼å®¹æ€§ä»£ç 
                content = compatibility_code + content
            
            # ä¿®å¤ all_gather ä½¿ç”¨æ–¹å¼
            if 'torch.distributed.nn.all_gather' in content:
                print(f"ğŸ“ ä¿®å¤ {file_path} ä¸­çš„ all_gather å‡½æ•°")
                content = re.sub(
                    r'torch\.distributed\.nn\.all_gather\(',
                    'torch.distributed.all_gather(',
                    content
                )
            
            # å¦‚æœå†…å®¹æœ‰å˜åŒ–ï¼Œå†™å›æ–‡ä»¶
            if content != original_content:
                # åˆ›å»ºå¤‡ä»½
                backup_path = file_path + '.backup_distributed_fix'
                shutil.copy2(file_path, backup_path)
                
                # å†™å…¥ä¿®å¤åçš„å†…å®¹
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                fixed_files.append(file_path)
                print(f"âœ… å·²ä¿®å¤: {file_path}")
                
        except Exception as e:
            print(f"âš ï¸  å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
    
    return fixed_files

def fix_shell_scripts():
    """ä¿®å¤shellè„šæœ¬ä¸­çš„å¯åŠ¨å‘½ä»¤"""
    
    script_files = []
    
    # æœç´¢ç›®å½•
    search_dirs = [
        "/root/autodl-tmp/Chinese-CLIP",
        "/root/autodl-tmp/cn_clip",
    ]
    
    for search_dir in search_dirs:
        if os.path.exists(search_dir):
            for root, dirs, files in os.walk(search_dir):
                for file in files:
                    if file.endswith('.sh'):
                        script_files.append(os.path.join(root, file))
    
    print(f"ğŸ” æ‰¾åˆ° {len(script_files)} ä¸ªshellè„šæœ¬éœ€è¦æ£€æŸ¥")
    
    fixed_scripts = []
    
    for script_path in script_files:
        try:
            with open(script_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            original_content = content
            
            # æ›¿æ¢å¯åŠ¨å‘½ä»¤
            if 'python3 -m torch.distributed.launch' in content:
                print(f"ğŸ“ ä¿®å¤ {script_path} ä¸­çš„å¯åŠ¨å‘½ä»¤")
                content = re.sub(
                    r'python3 -m torch\.distributed\.launch',
                    'torchrun',
                    content
                )
            
            if content != original_content:
                # åˆ›å»ºå¤‡ä»½
                backup_path = script_path + '.backup_launch_fix'
                shutil.copy2(script_path, backup_path)
                
                # å†™å…¥ä¿®å¤åçš„å†…å®¹
                with open(script_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                fixed_scripts.append(script_path)
                print(f"âœ… å·²ä¿®å¤: {script_path}")
                
        except Exception as e:
            print(f"âš ï¸  å¤„ç†è„šæœ¬ {script_path} æ—¶å‡ºé”™: {e}")
    
    return fixed_scripts

def main():
    print("=" * 60)
    print("  å®Œæ•´ä¿®å¤ torch.distributed.nn é—®é¢˜")
    print("=" * 60)
    print("ğŸ¯ ä¿®å¤å†…å®¹:")
    print("  1. ä¿®å¤è¯­æ³•é”™è¯¯")
    print("  2. torch.distributed.nn æ¨¡å—å¯¼å…¥é—®é¢˜")
    print("  3. all_gather å‡½æ•°ä½¿ç”¨é—®é¢˜") 
    print("  4. æ·»åŠ  PyTorch 1.12.0 å…¼å®¹å±‚")
    print("  5. ä¿®å¤shellè„šæœ¬å¯åŠ¨å‘½ä»¤")
    print("=" * 60)
    print()
    
    # æ­¥éª¤0: å…ˆä¿®å¤è¯­æ³•é”™è¯¯
    print("ğŸ“‹ æ­¥éª¤0: ä¿®å¤è¯­æ³•é”™è¯¯...")
    syntax_fixed = fix_broken_syntax()
    
    # æ­¥éª¤1: ä¿®å¤Pythonä»£ç 
    print("ğŸ“‹ æ­¥éª¤1: ä¿®å¤ torch.distributed.nn å¯¼å…¥...")
    fixed_files = fix_torch_distributed_nn_imports()
    
    # æ­¥éª¤2: ä¿®å¤shellè„šæœ¬
    print("ğŸ“‹ æ­¥éª¤2: ä¿®å¤shellè„šæœ¬å¯åŠ¨å‘½ä»¤...")
    fixed_scripts = fix_shell_scripts()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ä¿®å¤å®Œæˆ!")
    print("=" * 60)
    if syntax_fixed:
        print("âœ… è¯­æ³•é”™è¯¯å·²ä¿®å¤")
    print(f"âœ… ä¿®å¤äº† {len(fixed_files)} ä¸ªPythonæ–‡ä»¶")
    print(f"âœ… ä¿®å¤äº† {len(fixed_scripts)} ä¸ªshellè„šæœ¬")
    
    print("\nğŸ§ª å»ºè®®æµ‹è¯•:")
    print("  python -c 'import torch.distributed.nn; print(\"å¯¼å…¥æˆåŠŸ\")'")
    print("  python -c 'from cn_clip.training.train import train; print(\"è®­ç»ƒæ¨¡å—å¯¼å…¥æˆåŠŸ\")'")

if __name__ == "__main__":
    main()
EOF

# ä¸Šä¼ ä¿®å¤è„šæœ¬åˆ°æœåŠ¡å™¨
echo "ğŸ“¤ ä¸Šä¼ ä¿®å¤è„šæœ¬..."
scp /tmp/fix_torch_complete_remote.py $SERVER:/tmp/

# åœ¨æœåŠ¡å™¨ä¸Šæ‰§è¡Œä¿®å¤
echo ""
echo "ğŸš€ åœ¨æœåŠ¡å™¨ä¸Šæ‰§è¡Œå®Œæ•´ä¿®å¤..."
echo "=================================================="

ssh $SERVER << 'ENDSSH'
echo "ğŸ”„ æ¿€æ´»condaç¯å¢ƒ..."
source ~/.bashrc
conda activate base

echo ""
echo "ğŸ“ å½“å‰ç¯å¢ƒä¿¡æ¯:"
python --version
which python
echo "å·¥ä½œç›®å½•: $(pwd)"

echo ""
echo "ğŸš€ å¼€å§‹æ‰§è¡Œä¿®å¤..."
cd /root/autodl-tmp
python /tmp/fix_torch_complete_remote.py

echo ""
echo "ğŸ§ª æµ‹è¯•ä¿®å¤ç»“æœ..."
echo "æµ‹è¯•1: torch.distributed.nnå¯¼å…¥"
python -c "
try:
    import torch.distributed.nn
    print('âœ… torch.distributed.nn å¯¼å…¥æˆåŠŸ')
except Exception as e:
    print(f'âŒ å¯¼å…¥å¤±è´¥: {e}')
"

echo ""
echo "æµ‹è¯•2: trainæ¨¡å—å¯¼å…¥"
python -c "
try:
    import sys
    sys.path.append('/root/autodl-tmp/Chinese-CLIP')
    from cn_clip.training.train import train
    print('âœ… train æ¨¡å—å¯¼å…¥æˆåŠŸ')
except Exception as e:
    print(f'âŒ trainæ¨¡å—å¯¼å…¥å¤±è´¥: {e}')
"

echo ""
echo "æµ‹è¯•3: æ£€æŸ¥ä¿®å¤åçš„æ–‡ä»¶"
echo "ğŸ“ æ£€æŸ¥train.pyæ–‡ä»¶:"
head -20 /root/autodl-tmp/Chinese-CLIP/cn_clip/training/train.py

ENDSSH

echo ""
echo "=================================================="
echo "ğŸ‰ äº‘ç«¯ä¿®å¤æ‰§è¡Œå®Œæˆï¼"
echo "=================================================="
echo -e "${GREEN}âœ… ä¿®å¤ä»»åŠ¡å·²åœ¨ $SERVER æœåŠ¡å™¨ä¸Šå®Œæˆ${NC}"
echo ""
echo "ğŸ“‹ ä¿®å¤å†…å®¹:"
echo "  âœ… è¯­æ³•é”™è¯¯ä¿®å¤"
echo "  âœ… torch.distributed.nn å…¼å®¹å±‚"
echo "  âœ… all_gather å‡½æ•°ä¿®å¤"
echo "  âœ… shellè„šæœ¬å¯åŠ¨å‘½ä»¤ä¿®å¤"
echo ""
echo "ğŸš€ ä¸‹ä¸€æ­¥:"
echo "  1. SSHåˆ°æœåŠ¡å™¨æµ‹è¯•è®­ç»ƒ"
echo "  2. è¿è¡Œå•å¡æµ‹è¯•éªŒè¯ä¿®å¤æ•ˆæœ"
echo "  3. å¦‚æœ‰é—®é¢˜æŸ¥çœ‹å¤‡ä»½æ–‡ä»¶æ¢å¤"
echo ""
echo "ğŸ”— è¿æ¥åˆ°æœåŠ¡å™¨:"
echo "  ssh $SERVER"
echo ""

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm -f /tmp/fix_torch_complete_remote.py

echo "ğŸ§¹ æœ¬åœ°ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†" 